{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('inaugural')\n",
    "\n",
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inaugural.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 58 inaugural speech text files in this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract file names\n",
    "filenames = inaugural.fileids()\n",
    "\n",
    "filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the speech text from a file\n",
    "\n",
    "inaugural.raw(filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_punctuations = set(string.punctuation)\n",
    "\n",
    "main_speech = {}\n",
    "\n",
    "for filename in filenames:\n",
    "    \n",
    "    name = filename.split('.')[0]\n",
    "    \n",
    "    print ('Processing:', name)\n",
    "\n",
    "    # read each line from the file and convert it into lowercase\n",
    "    line = inaugural.raw(filename).lower()\n",
    "\n",
    "    # remove all punctuations\n",
    "    line_clean = ''.join(l for l in line if l not in all_punctuations)\n",
    "\n",
    "    # remove all stop words (this will create a list of words)\n",
    "    # in addition, use regex to remove non-alphabetic characters\n",
    "    line_words = [re.sub(\"[^a-zA-Z' ]+\", '', word) for word in line_clean.split() if word not in ENGLISH_STOP_WORDS\n",
    "                 and word != 'applause']\n",
    "\n",
    "    # join all those words to create a line (of text) again\n",
    "    main_speech[name] = ' '.join(line_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length (number of words) of each speech\n",
    "for name, speech in main_speech.items():\n",
    "    \n",
    "    print (name, len(speech))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "speech_lengths = []\n",
    "\n",
    "for name, speech in main_speech.items():\n",
    "    \n",
    "    names.append(name)\n",
    "    speech_lengths.append(len(speech))\n",
    "    \n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "plt.figure().set_size_inches(9, 12)\n",
    "\n",
    "sns.barplot(y=names, x=speech_lengths, color='seagreen')\n",
    "\n",
    "plt.xlabel('Number of words', fontsize=14);\n",
    "#plt.xticks(rotation='vertical');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "tfs_matrix = tfidf.fit_transform(main_speech.values())\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "\n",
    "scores = tfs_matrix.todense().tolist()\n",
    "\n",
    "df = pd.DataFrame(scores, columns=feature_names, index=main_speech.keys())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset can be used for further exploratory analysis, e.g., trends, cluster analysis, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most distinct words by speech\n",
    "\n",
    "df.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between the 10 most recent speeches\n",
    "corr_matrix = df.tail(10).T.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "cmap = sns.diverging_palette(10, 220, n=20)\n",
    "\n",
    "ax = sns.heatmap(corr_matrix, cmap=cmap)\n",
    "ax.set_ylim(len(corr_matrix), 0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=np.bool))\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "cmap = sns.diverging_palette(10, 220, n=20)\n",
    "\n",
    "ax = sns.heatmap(corr_matrix, cmap=cmap, mask=mask, vmax=.4)\n",
    "ax.set_ylim(len(corr_matrix), 0);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
