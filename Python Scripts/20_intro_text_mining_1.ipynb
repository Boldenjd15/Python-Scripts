{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Speech: \"Friends, Romans, countrymen, lend me your ears\" BY WILLIAM SHAKESPEARE](https://www.poetryfoundation.org/poems/56968/speech-friends-romans-countrymen-lend-me-your-ears)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = 'Friends, Romans, countrymen, lend me your ears; \\\n",
    "I come to bury Caesar, not to praise him. \\\n",
    "The evil that men do lives after them; \\\n",
    "The good is oft interred with their bones; \\\n",
    "So let it be with Caesar. The noble Brutus \\\n",
    "Hath told you Caesar was ambitious: \\\n",
    "If it were so, it was a grievous fault, \\\n",
    "And grievously hath Caesar answer’d it. \\\n",
    "Here, under leave of Brutus and the rest– \\\n",
    "For Brutus is an honourable man; \\\n",
    "So are they all, all honourable men– \\\n",
    "Come I to speak in Caesar’s funeral. \\\n",
    "He was my friend, faithful and just to me: \\\n",
    "But Brutus says he was ambitious; \\\n",
    "And Brutus is an honourable man. \\\n",
    "He hath brought many captives home to Rome \\\n",
    "Whose ransoms did the general coffers fill: \\\n",
    "Did this in Caesar seem ambitious? \\\n",
    "When that the poor have cried, Caesar hath wept: \\\n",
    "Ambition should be made of sterner stuff: \\\n",
    "Yet Brutus says he was ambitious; \\\n",
    "And Brutus is an honourable man. \\\n",
    "You all did see that on the Lupercal \\\n",
    "I thrice presented him a kingly crown, \\\n",
    "Which he did thrice refuse: was this ambition? \\\n",
    "Yet Brutus says he was ambitious; \\\n",
    "And, sure, he is an honourable man. \\\n",
    "I speak not to disprove what Brutus spoke, \\\n",
    "But here I am to speak what I do know. \\\n",
    "You all did love him once, not without cause: \\\n",
    "What cause withholds you then, to mourn for him? \\\n",
    "O judgment! thou art fled to brutish beasts, \\\n",
    "And men have lost their reason. Bear with me; \\\n",
    "My heart is in the coffin there with Caesar, \\\n",
    "And I must pause till it come back to me.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_freq = Counter(speech.split())\n",
    "\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort from high to low frequency words\n",
    "\n",
    "sorted(word_freq.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common way to deal with text documents is to first convert them into a numeric vector form (sparse matrix), and then perform additional analysis -- like clsutering, classification, and visualization -- using those vectors. This is usually referred to as 'Bag-of-Words' or 'Vector Space Model'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we convert the text into a numeric vector form, we should clean it up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_punctuations = set(string.punctuation)\n",
    "\n",
    "all_punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_clean = ''.join(l for l in speech if l not in all_punctuations)\n",
    "\n",
    "speech_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cover to Upper/Lower-case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_clean = speech_clean.lower()\n",
    "\n",
    "speech_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no standard definition of \"stop words\", but in general, it usually refers to most common words, like 'a', 'the', 'at'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scikit-learn` package provides a list of stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discard all stop words from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_words = [word for word in speech_clean.split() if word not in ENGLISH_STOP_WORDS]\n",
    "\n",
    "set(speech_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing inflected or derived words to their word stem, base or root form. There are several stemming algorithms available; we will use *Porter* and *Lancaster* stemmers in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# initilize the stemmer\n",
    "stemmer= PorterStemmer()\n",
    "\n",
    "# example 1\n",
    "for word in ['Play', 'Playing', 'Played']:\n",
    "    \n",
    "    stem = stemmer.stem(word)\n",
    "    \n",
    "    print ('Word:', word, '\\t --> Stem:', stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 2\n",
    "\n",
    "for word in ['grievous', 'grievously']:\n",
    "    \n",
    "    stem = stemmer.stem(word)\n",
    "    \n",
    "    print ('Word:', word, ' \\t --> Stem:', stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# initialize\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "for word in ['grievous', 'grievously']:\n",
    "    \n",
    "    stem = stemmer.stem(word)\n",
    "    \n",
    "    print ('Word:', word, ' \\t --> Stem:', stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply stemmer on the speech text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty array to store the results (i.e., stems)\n",
    "stems = []\n",
    "\n",
    "for word in speech_words:\n",
    "    \n",
    "    # check if it's a stop word\n",
    "    if word not in ENGLISH_STOP_WORDS:\n",
    "        \n",
    "        # append the stem for each word to the output array\n",
    "        stems.append(stemmer.stem(word))\n",
    "   \n",
    "set(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: [Julie Beth Lovins](https://en.wikipedia.org/wiki/Julie_Beth_Lovins), a computational linguist, published the first-ever stemming algorithm in 1968.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Stemming_ usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. _Lemmatization_ usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. [[source]](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# initialize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word in ['know', 'knowing', 'knew', 'knowledge']:\n",
    "    \n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    stem = stemmer.stem(word)\n",
    "    \n",
    "    print ('Word:', word, '--> Stem:', stem, '--> Lemma:', lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must provide the context in which we're trying to lemmatize the words. This is refered to as the Parts-Of-Speech (POS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in ['know', 'knowing', 'knew', 'knowledge']:\n",
    "    \n",
    "    # adding pos argument to lemmatize()\n",
    "    lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "    stem = stemmer.stem(word)\n",
    "    \n",
    "    print ('Word:', word, '--> Stem:', stem, '--> Lemma:', lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty array to store the results (i.e., lemmas)\n",
    "lemmas = []\n",
    "\n",
    "for word in speech_words:\n",
    "    \n",
    "    # check if it's a stop word\n",
    "    if word not in ENGLISH_STOP_WORDS:\n",
    "        \n",
    "        # append the stem for each word to the output array\n",
    "        lemmas.append(lemmatizer.lemmatize(word, 'v'))\n",
    "    \n",
    "set(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speech_clean.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization are closely related. Unlike Lemmatization, Stemming doesn't incorporate the conext (part of speech) but they typically run faster. In Information Retrieval applications, Stemming improves the True Positive Rate (recall), but reduces the True Negative Rate (specificity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next part of this exercise, let's analyze transcripts from a couple of US presidential inagural addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the filepath to your local machine where the speech text files are located\n",
    "\n",
    "trump_speech_transcript = r\"C:\\Users\\visha\\derive Dropbox\\clients\\vcu\\python\\misc\\inaugural_speech_trump.txt\"\n",
    "obama_speech_transcript = r\"C:\\Users\\visha\\derive Dropbox\\clients\\vcu\\python\\misc\\inaugural_speech_obama.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the NLTK tokenizer to split a sentence into words. More details details about NLTK can be found [here](https://www.nltk.org/_modules/nltk/tokenize/punkt.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokens(infile):\n",
    "    \n",
    "    with open(infile) as f:\n",
    "        \n",
    "        # read each line from the file and convert it into lowercase\n",
    "        line = f.read().lower()\n",
    "\n",
    "        # remove all punctuations\n",
    "        line_clean = ''.join(l for l in line if l not in all_punctuations)\n",
    "        \n",
    "        # remove all stop words (this will create a list of words)\n",
    "        line_words = [word for word in line_clean.split() if word not in ENGLISH_STOP_WORDS]\n",
    "\n",
    "        # join all those words to create a line (of text) again\n",
    "        line_clean = ' '.join(line_words)\n",
    "\n",
    "        # tokenize\n",
    "        tokens = nltk.word_tokenize(line_clean)\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "tokens = create_tokens(trump_speech_transcript)\n",
    "\n",
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(tokens)\n",
    "\n",
    "print (count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some non-alphabetical characters that were not captured by the `all_punctuations` filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def create_tokens(infile):\n",
    "    \n",
    "    with open(infile) as f:\n",
    "        \n",
    "        # read each line from the file and convert it into lowercase\n",
    "        line = f.read().lower()\n",
    "\n",
    "        # remove all punctuations\n",
    "        line_clean = ''.join(l for l in line if l not in all_punctuations)\n",
    "        \n",
    "        # remove all stop words (this will create a list of words)\n",
    "        # in addition, use regex to replace non-alphabetic characters into null\n",
    "        line_words = [re.sub(\"[^a-zA-Z]\", '', word) for word in line_clean.split() if word not in ENGLISH_STOP_WORDS]\n",
    "        \n",
    "        # join all those words to create a line (of text) again\n",
    "        line_clean = ' '.join(line_words)\n",
    "\n",
    "        # tokenize\n",
    "        tokens = nltk.word_tokenize(line_clean)\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "tokens_trump = create_tokens(trump_speech_transcript)\n",
    "\n",
    "token_count_trump = Counter(tokens_trump)\n",
    "\n",
    "print (token_count_trump.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: For an explanation of how that `regex` query replaces all non-letter chatacters with '' (nothing), please follow this [link](https://stackoverflow.com/questions/47561298/python-regex-remove-escape-characters-and-punctuation-except-for-apostrophe?rq=1).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create tokens from Obama's speeach\n",
    "\n",
    "tokens_obama = create_tokens(obama_speech_transcript)\n",
    "\n",
    "token_count_obama = Counter(tokens_obama)\n",
    "\n",
    "print (token_count_obama.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remove 'applause' from this list as it's not part of the speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokens(infile):\n",
    "    \n",
    "    with open(infile) as f:\n",
    "        \n",
    "        # read each line from the file and convert it into lowercase\n",
    "        line = f.read().lower()\n",
    "\n",
    "        # remove all punctuations\n",
    "        line_clean = ''.join(l for l in line if l not in all_punctuations)\n",
    "        \n",
    "        # remove all stop words (this will create a list of words)\n",
    "        # in addition, use regex to replace \n",
    "        line_words = [re.sub(\"[^a-zA-Z' ]+\", '', word) for word in line_clean.split() if word not in ENGLISH_STOP_WORDS\n",
    "                     and word != 'applause']\n",
    "\n",
    "        # join all those words to create a line (of text) again\n",
    "        line_clean = ' '.join(line_words)\n",
    "\n",
    "        # tokenize\n",
    "        tokens = nltk.word_tokenize(line_clean)\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "tokens_obama = create_tokens(obama_speech_transcript)\n",
    "\n",
    "token_count_obama = Counter(tokens_obama)\n",
    "\n",
    "print (token_count_obama.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF stands for Term Frequency – Inverse Document Frequency. The idea behind this metric is to rescale the frequency of each word by how often they appear across all documents. Words that are common across all documents are penalized, and as a result, the words that are most distinct (and ferquent) within a document are emphasized more. Read more about TF-IDF [here](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's store all tokens in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tokens_all = {}\n",
    "\n",
    "with open(trump_speech_transcript) as f:\n",
    "\n",
    "    # read each line from the file and convert it into lowercase\n",
    "    line = f.read().lower()\n",
    "\n",
    "    # remove all punctuations\n",
    "    line_clean = ''.join(l for l in line if l not in all_punctuations)\n",
    "\n",
    "    # remove all stop words (this will create a list of words)\n",
    "    # in addition, use regex to remove non-alphabetic characters\n",
    "    line_words = [re.sub(\"[^a-zA-Z' ]+\", '', word) for word in line_clean.split() if word not in ENGLISH_STOP_WORDS\n",
    "                 and word != 'applause']\n",
    "\n",
    "    # join all those words to create a line (of text) again\n",
    "    line_clean = ' '.join(line_words)\n",
    "\n",
    "    tokens_all['trump'] = line_clean\n",
    "    \n",
    "with open(obama_speech_transcript) as f:\n",
    "\n",
    "    # read each line from the file and convert it into lowercase\n",
    "    line = f.read().lower()\n",
    "\n",
    "    # remove all punctuations\n",
    "    line_clean = ''.join(l for l in line if l not in all_punctuations)\n",
    "\n",
    "    # remove all stop words (this will create a list of words)\n",
    "    # in addition, use regex to replace \n",
    "    line_words = [re.sub(\"[^a-zA-Z' ]+\", '', word) for word in line_clean.split() if word not in ENGLISH_STOP_WORDS\n",
    "                 and word != 'applause']\n",
    "\n",
    "    # join all those words to create a line (of text) again\n",
    "    line_clean = ' '.join(line_words)\n",
    "\n",
    "    tokens_all['obama'] = line_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_all.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "tfs_matrix = tfidf.fit_transform(tokens_all.values())\n",
    "\n",
    "print(tfs_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how a sparse matrix is represented in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature (\"column\") names\n",
    "\n",
    "print(tfidf.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert from sparse matrix to dense matrix\n",
    "\n",
    "tfs_matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "\n",
    "scores = tfs_matrix.todense().tolist()\n",
    "\n",
    "df = pd.DataFrame(scores, columns=feature_names, index=['trump', 'obama'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select the top 10 most _common_ words from Trump's speech and then we will select those columns from the above data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count_trump.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can iterate through this list of words\n",
    "for w, c in token_count_trump.most_common(10):\n",
    "    print (w, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's store these words in an array\n",
    "words_trump = []\n",
    "\n",
    "for w, c in token_count_trump.most_common(10):\n",
    "    words_trump.append(w)\n",
    "    \n",
    "words_trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[words_trump].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words 'america' and 'american' the most commonly used words by Trump in his inaugural speech, and these were also _distinct_ words used by Trump as compared to Obama. (In other words, Obama didn't used these two words as frequently as Trump did.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, let's select the top 10 most common words from Obama's speech and then select those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_obama = []\n",
    "\n",
    "for w, c in token_count_obama.most_common(10):\n",
    "    words_obama.append(w)\n",
    "    \n",
    "words_obama\n",
    "\n",
    "df[words_obama].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words such as 'freedom', 'equal', and 'journey' were some of the most commonly used words by Obama in his inaugural speech, while Trump did not use these words even once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Text from Web-pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following web-site contains US Presidential inauguration speeches: http://avalon.law.yale.edu/subject_menus/inaug.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `requests` and `BeautifulSoup` packages to read data directly from this web-site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'http://avalon.law.yale.edu/21st_century/obama.asp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Ping the web-page for information. This called making a request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_code = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Use Beautiful Soup to parse the document using the best available parser. It will use an HTML parser unless you specifically tell it to use an XML parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(source_code.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the content\n",
    "\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Extract the text part of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_obama = soup.get_text()\n",
    "\n",
    "speech_obama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Extract a specific portion of the text chunk which contains the actual speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the speech starts with 'My fellow citizens', which is immediately preceeded by the following: `\\r\\n\\n\\n\\n`. Let's split the text chunk into two parts using `\\r\\n\\n\\n\\n` as the separator, and then take the second half of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_obama = speech_obama.split('\\r\\n\\n\\n\\n')[1]\n",
    "\n",
    "speech_obama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the speech actually ends with 'And God bless the United States of America.', which is immediately followed by `\\n\\n\\n\\n\\n`. Let's split the text chunk into two parts using `\\n\\n\\n\\n\\n` as the separator, and then take the *first* half of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_obama = speech_obama.split('\\n\\n\\n\\n')[0]\n",
    "\n",
    "speech_obama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Clean and Tokenize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read each line from the file and convert it into lowercase\n",
    "line = speech_obama.lower()\n",
    "\n",
    "# remove all punctuations\n",
    "line_clean = ''.join(l for l in line if l not in all_punctuations)\n",
    "\n",
    "# remove all stop words (this will create a list of words)\n",
    "line_words = [word for word in line_clean.split() if word not in ENGLISH_STOP_WORDS]\n",
    "\n",
    "# join all those words to create a line (of text) again\n",
    "line_clean = ' '.join(line_words)\n",
    "\n",
    "# tokenize\n",
    "tokens = nltk.word_tokenize(line_clean)\n",
    "\n",
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applications of Text Mining:**\n",
    "    \n",
    "    1. Text (or Document) Categorization\n",
    "    2. Text Clustering\n",
    "    3. Sentiment Analysis\n",
    "    4. Document Summarization\n",
    "    5. Topic Extraction\n",
    "    6. Document Associations \n",
    "    7. Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources:**\n",
    "    \n",
    "1. [NLTK 3.4 documentation](http://www.nltk.org/index.html)\n",
    "2. [spaCy API](https://spacy.io/api)\n",
    "3. [scikit-learn TF-IDF Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "4. [NLTK's WordNet Interface](http://www.nltk.org/howto/wordnet.html)\n",
    "5. [Modern NLP in Python by Patrick Harrison | PyData DC 2016](https://www.youtube.com/watch?v=6zm9NC9uRkk) (YouTube)\n",
    "6. [Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/) (Book)\n",
    "7. [Text Feature Extraction using scikit-learn](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
